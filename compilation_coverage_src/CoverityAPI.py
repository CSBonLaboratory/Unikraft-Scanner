#from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver import ActionChains
from seleniumwire import webdriver
from seleniumwire.webdriver import Firefox
from seleniumwire.request import Request, Response
from helpers.InterceptorTimeout import InterceptorTimeout
from selenium.common.exceptions import TimeoutException
from seleniumwire.utils import decode
import os
import json
import logging
import subprocess
from threading import Condition
from enum import Enum
from coverage import LOGGER_NAME
import re



logger = logging.getLogger(LOGGER_NAME)

#TODO: remove logs generated by Selenium

class RecentSnapshotStates(Enum):
    NOT_STARTED = 0
    STARTED_BUT_NOT_FOUND = 1
    FOUND = 2

class DefectsStates(Enum):
    NOT_STARTED = 0
    STARTED_BUT_NOT_FOUND = 1
    FOUND = 2


def dummy_request_interceptor(request : Request):
    return

def recent_snapshot_response_interceptor(request : Request, response : Response):

    coverityAPI = CoverityAPI(None)

    if coverityAPI.recent_snapshot_state in [RecentSnapshotStates.NOT_STARTED, RecentSnapshotStates.FOUND]:
        logger.debug(f"RECENT SNAPSHOT: Ignoring {request.url} for recent snapshot since state is {coverityAPI.recent_snapshot_state.name}")
        return

    logger.debug(f"RECENT SNAPSHOT: Investigating {request.url}")

    if request.url == coverityAPI.snapshots_table_url:
        logger.debug(f"RECENT SNAPSHOT: Found snapshot table at {request.url}")
        with coverityAPI.interceptor_condition:
            
            # https://stackoverflow.com/questions/67306915/selenium-wire-response-object-way-to-get-response-body-as-string-rather-than-b
            snapshot_payload = json.loads(decode(response.body, response.headers.get('Content-Encoding', 'identity')))['resultSet']['results']

            if snapshot_payload == []:
                coverityAPI.cached_recent_snapshot = None
            else:
                coverityAPI.cached_recent_snapshot = snapshot_payload[0]

            logger.debug(f"Found most recent snapshot {coverityAPI.cached_recent_snapshot}")

            coverityAPI.recent_snapshot_state = RecentSnapshotStates.FOUND

            coverityAPI.interceptor_condition.notify()

    return

def defects_response_interceptor(request : Request, response : Response):

    coverityAPI : CoverityAPI = CoverityAPI(None)

    logger.debug(f"RECENT DEFECTS: Investigating {request.url}")

    if coverityAPI.defects_state in [DefectsStates.NOT_STARTED, DefectsStates.FOUND]:
        logger.debug(f"DEFECTS: Ignoring {request.url} for recent defects since status is {coverityAPI.defects_state.name}")
        return
    
    logger.debug(f"RECENT DEFECTS: Investigating {request.url}")
    current_defect_view_match = re.search(r"https://scan9\.scan\.coverity\.com/reports/table\.json\?projectId=(\d+)&viewId=(\d+)", request.url)

    if current_defect_view_match:
        
        coverityAPI.defects_copy_view_id = current_defect_view_match.group(2)

        logger.debug(f"RECENT DEFECTS: Found copy of the defects view with id {coverityAPI.defects_copy_view_id} at {request.url}")

        with coverityAPI.interceptor_condition:

            coverityAPI.cached_last_defect_results = json.loads(decode(response.body, response.headers.get('Content-Encoding', 'identity')))['resultSet']['results']

            logger.debug(f"Found last defects {coverityAPI.cached_last_defect_results}")

            coverityAPI.defects_state = DefectsStates.FOUND

            coverityAPI.interceptor_condition.notify()
    
    return

class CoverityAPI(object):

    user_email : str
    user_pass : str
    upload_token : str
    browser : Firefox
    scraper_wait : int
    project_overview_url : str
    project_id : int
    snapshots_url : str
    snapshots_table_url : str
    snapshots_view_id : int
    defects_url : str
    defects_table_url : str
    defects_view_id : int
    defects_copy_view_id : int
    recent_snapshot_state : RecentSnapshotStates
    defects_state : DefectsStates
    interceptor_condition : Condition
    cached_last_defect_results : list[dict]
    cached_recent_snapshot : dict
    interceptor_timeout : int
    is_auth : bool
    log_file : str
    upload_token : str

    def __new__(cls, configs : dict | None) -> None:
        if not hasattr(cls, 'instance') and configs != None:
            
            cls.instance : CoverityAPI = super(CoverityAPI, cls).__new__(cls)

            cls.instance.log_file = configs['logfile']
            cls.instance.upload_token = configs['coverityAPI']['uploadToken']
            
            cls.instance.options = Options()
            cls.instance.options.binary_location = configs["coverityAPI"]["firefoxPath"]
            gecko_service = Service(configs["coverityAPI"]["geckoPath"])
            cls.instance.browser = webdriver.Firefox(options=cls.instance.options, service=gecko_service)

            cls.instance.interceptor_condition = Condition()

            cls.instance.browser.request_interceptor = dummy_request_interceptor

            cls.instance.recent_snapshot_state = RecentSnapshotStates.NOT_STARTED
            cls.instance.defects_state = DefectsStates.NOT_STARTED

            cls.instance.snapshots_view_id = configs["coverityAPI"]["snapshotsViewId"]
            cls.instance.project_id = configs["coverityAPI"]["projectId"]
            cls.instance.defects_view_id = configs["coverityAPI"]["defectsViewId"]

            # this url is accessible and will trigger a request to the second one which contains snapshots data
            cls.instance.snapshots_url = f"https://scan9.scan.coverity.com/#/project-view/{cls.instance.snapshots_view_id}/{cls.instance.project_id}"
            cls.instance.snapshots_table_url = f"https://scan9.scan.coverity.com/reports/table.json?projectId={cls.instance.project_id}&viewId={cls.instance.snapshots_view_id}"

            # this url is accessible and will trigger a request to the second one which contains defects data for the selected snapshot
            cls.instance.defects_url = f"https://scan9.scan.coverity.com/#/project-view/{cls.instance.defects_view_id}/{cls.instance.project_id}"
            cls.instance.defects_table_url = f"https://scan9.scan.coverity.com/reports/table.json?projectId={cls.instance.project_id}&viewId={cls.instance.defects_view_id}"

            

            cls.instance.scraper_wait = configs["coverityAPI"]["scraperWaitSeconds"]
            cls.instance.project_overview_url = configs["coverityAPI"]["projectOverviewURL"]

            cls.instance.interceptor_timeout = configs["coverityAPI"]["interceptorTimeoutSeconds"]

            cls.instance.is_auth = False

            logger.debug(f"SNAPSHOT TABLE URL: {cls.instance.snapshots_table_url}")
            logger.debug(f"SNAPSHOTS URL: {cls.instance.snapshots_url}")

            logger.debug(f"DEFECTS TABLE URL: {cls.instance.defects_table_url}")
            logger.debug(f"DEFECTS URL: {cls.instance.defects_url}")

        return cls.instance
    
    def auth(self):

        browser = self.browser

        browser.switch_to.new_window('tab')

        browser.get(self.project_overview_url)

        try:
            # wait until "View Defects" button can be seen and clicked. This means that we wait until the user MANUALLY logs in and solves the captcha
            WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, "/html/body/div[1]/div[2]/div/div/div[3]/div[1]/p[1]/a")))
        except TimeoutException as te:
            self.is_auth = False
            raise te
        
        self.is_auth = True

        return

    def prepare_defects_for_db(self, defect : dict, compilation_tag) -> dict:

        # just remove the / from the beggining of the path, Coverity adds its since the archive submited is considered to be the whole filesystem
        defect['displayFile'] = defect['displayFile'][1 : ]

        # add compilation tag to the defect dict
        defect['compilation_tag'] = compilation_tag

        return defect
    
    def check_recent_snapshot(self, compilation_tag : str) -> bool:

        if not self.is_auth:
            logger.warning("Authenticating before checking the most recent snapshot")
            try:
                self.auth()
            except TimeoutException as te:
                logger.critical("Authentication failed during fetching the most recent snapshot")
                raise te

        browser = self.browser
        
        logger.debug("Start intercepting for snapshot resultSet response")

        # init interceptor response body which contains all snapshots information
        self.recent_snapshot_state = RecentSnapshotStates.STARTED_BUT_NOT_FOUND
        browser.response_interceptor = recent_snapshot_response_interceptor
        
        browser.switch_to.new_window('tab')
        
        logger.debug(f"Access {self.snapshots_url} in order to get snapshots resultSet")
        browser.get(self.snapshots_url)

        # wait until the interceptor finds the response that contains the snapshot details
        with self.interceptor_condition:
            while self.recent_snapshot_state != RecentSnapshotStates.FOUND:
                self.interceptor_condition.wait(self.interceptor_timeout)

                # timeout inside the interceptor
                if self.recent_snapshot_state != RecentSnapshotStates.FOUND:
                    logger.critical(f"Interceptor timeout. Expected {RecentSnapshotStates.FOUND.name}, got {self.recent_snapshot_state.name}")
                    raise InterceptorTimeout()
                
            # reinit the state of the snapshots interceptor maybe for further use
            self.recent_snapshot_state = RecentSnapshotStates.NOT_STARTED

        if self.cached_recent_snapshot['snapshotDescription'] != compilation_tag:
            logger.warning(f"Recent snapshot has compilation tag \"{self.cached_recent_snapshot['snapshotDescription']}\", expected \"{compilation_tag}\". Needs retrying.")
            return False
        
        return True

    def fetch_and_cache_recent_defects(self, compilation_tag : str) -> None:
        
        import time

        if not self.is_auth:
            logger.warn("Authenticating before fetching most recent defects...")
            try:
                self.auth()
            except TimeoutException as te:
                logger.critical("Authentication failed during fetching defects in the most recent snapshot")
                raise te
            
        browser = self.browser

        # try to intercept response body which contains all snapshot information
        browser.switch_to.new_window('tab')

        browser.get(self.snapshots_url)
        
        try:
            recent_snapshot_cell = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/cim-root/cim-shell/div/cim-project-view/div[1]/div[1]/div[1]/cim-data-table/angular-slickgrid/div/div/div[4]/div[3]/div/div[1]/div[2]')))
            double_click_snapshot_cell = ActionChains(browser).double_click(recent_snapshot_cell).perform()
        except TimeoutException as te:
            logger.critical("Cannot find most recent snapshot cell when fetching defects")
            raise te
        time.sleep(2)
        # Add more columns
        try:
            edit_settings_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="current-selection-settings"]')))
            edit_settings_button.click()
        except TimeoutException as te:
                logger.critical("Cannot find `edit settings` button")
                raise te
        
        logger.debug("Opened settings in order to add additional columns in the snapshot view. Next click on `Save as Copy` button")
        time.sleep(2)

        try:
            save_as_copy_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/simple-modal-holder/simple-modal-wrapper/div/cim-setting-modal/cim-base-modal/div/div[2]/div/div/form/ul/li[2]/label')))
            save_as_copy_button.click()
        except TimeoutException as te:
            logger.critical("Cannot click on `Save as Copy` button")
            raise te

        logger.debug("Clicked on `Save as Copy` button. Next click no `Column` button")
        time.sleep(2)

        try:
            column_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/simple-modal-holder/simple-modal-wrapper/div/cim-setting-modal/cim-base-modal/div/div[2]/div/div/fieldset/div/ul/li[2]/a/span')))
            column_button.click()
        except TimeoutException as te:
            logger.critical("Cannot click on `Column` button")
            raise te

        logger.debug("Clicked on column button. Next choose `Checker` column")
        time.sleep(2)

        try:
            checker_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-checker"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", checker_column)
            time.sleep(1)
            checker_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `Checker` column")
            raise te

        logger.debug("Clicked on `Checker` column. Next choose `CWE` column")
        time.sleep(2)

        try:
            cwe_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-cwe"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", cwe_column)
            time.sleep(1)
            cwe_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `CWE` column")
            raise te
        
        logger.debug("Clicked on `CWE` column. Next choose `Line Number` column")
        time.sleep(2)

        try:
            line_number_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-lineNumber"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", line_number_column)
            time.sleep(1)
            line_number_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `Line Number` column")
            raise te
        
        logger.debug("Clicked on `Line Number` column. Next choose `Score` column")
        time.sleep(2)

        try:
            score_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-score"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", score_column)
            time.sleep(1)
            score_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `Score` column")
            raise te

        logger.debug("Clicked on `Score` column. Next enter name of the snapshot issue copy")
        time.sleep(2)

        try:
            snapshot_copy_input = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="view-name"]')))
            snapshot_copy_input.clear()
            snapshot_copy_input.send_keys(compilation_tag)
        except TimeoutException as te:
            logger.critical("Cannot input the name of the snapshot issue copy")
            raise te

        logger.debug("Chose a name for the snapshot issue copy. Next click on `OK` button to finish and start intercepting the response body for defects")
        time.sleep(2)

        self.defects_state = DefectsStates.STARTED_BUT_NOT_FOUND
        browser.response_interceptor = defects_response_interceptor

        try:
            ok_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/simple-modal-holder/simple-modal-wrapper/div/cim-setting-modal/cim-base-modal/div/div[3]/div/div/button[2]')))

            logger.debug("Init defects response interceptor")
            # only after initialising the interceptor, we can click on 'OK'
            ok_button.click()
        except TimeoutException as te:
            logger.critical("Cannot click on `OK` button to finish adding more columns")
            raise te

        logger.debug("Finished adding new columns. Now try to intercept the response ")

        with self.interceptor_condition:
            while self.defects_state != DefectsStates.FOUND:
                self.interceptor_condition.wait(self.interceptor_timeout)

                # timeout inside the interceptor
                if self.defects_state != DefectsStates.FOUND:
                    logger.critical(f"Interceptor timeout. Expected {DefectsStates.FOUND.name}, got {self.defects_state}")
                    raise InterceptorTimeout()
                
            # reinit the state of the defects interceptor maybe for further use
            self.defects_state = DefectsStates.NOT_STARTED
        

        return


    def submit_build(self, app_path : str, compile_cmd : str, compilation_tag : str) -> bool:
        
        upload_script_path = os.getcwd() + "/.."

        # make this env vars in order to pass them to the upload.sh script
        os.environ['UK_COV_APP_COMPILE_CMD'] = compile_cmd
        os.environ['UK_COV_APP_PATH'] = app_path
        os.environ['UK_COV_LOG_FILE'] = self.log_file
        os.environ['UK_COV_UPLOAD_TOKEN'] = self.upload_token
        os.environ['UK_COV_DESCRIPTION'] = compilation_tag

        job = f"{upload_script_path}/upload.sh"

        logger.debug(f"Passing environment parameters to upload script:\n {os.environ['UK_COV_APP_COMPILE_CMD']}\n{os.environ['UK_COV_APP_PATH']}\n")

        logger.debug(f"Executing upload job: {job}")

        submit_proc = subprocess.Popen(job, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)

        out_raw, err_raw = submit_proc.communicate()

        out = out_raw.decode()
        err = err_raw.decode()
        
        submit_proc.terminate()

        logger.info(out)
        logger.warning(err)

        with open(f"{app_path}/.cov-build-logs.txt", "rt") as f:
            cov_build_logs = f.read()
            f.close()
        
        logger.debug(cov_build_logs)

        os.remove(f"{app_path}/.cov-build-logs.txt")

        if "No files were emitted." in err:
            logger.critical("The app was previously compiled. Remove the .unikraft directory and try again")
            return False

        elif "C/C++ compilation units (100%) are ready for analysis" not in cov_build_logs:
            logger.critical("The app has compilation issues. All files presented in the kraft configuration should be able to be compiled")
            return False
        
        
        return True
                    
