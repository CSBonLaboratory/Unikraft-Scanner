#from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver import ActionChains
from seleniumwire import webdriver
from seleniumwire.webdriver import Firefox
from seleniumwire.request import Request, Response
from helpers.InterceptorTimeout import InterceptorTimeout
from selenium.common.exceptions import TimeoutException
from seleniumwire.utils import decode
import os
import json
import logging
import subprocess
from threading import Condition
from enum import Enum
from coverage import LOGGER_NAME
import re
import shutil
import time



logger = logging.getLogger(LOGGER_NAME)

#TODO: remove logs generated by Selenium

class RecentSnapshotStates(Enum):
    NOT_STARTED = 0
    STARTED_BUT_NOT_FOUND = 1
    FOUND = 2

class DefectsStates(Enum):
    NOT_STARTED = 0
    STARTED_BUT_NOT_FOUND = 1
    FOUND = 2


def dummy_request_interceptor(request : Request):
    return

def dummy_response_interceptor(request : Request, response : Response):
    return

def recent_snapshot_response_interceptor(request : Request, response : Response):

    coverityAPI : CoverityAPI = CoverityAPI(None)

    if coverityAPI.recent_snapshot_state in [RecentSnapshotStates.NOT_STARTED, RecentSnapshotStates.FOUND]:
        logger.debug(f"RECENT SNAPSHOT: Ignoring {request.url} for recent snapshot since state is {coverityAPI.recent_snapshot_state.name}")
        return

    logger.debug(f"RECENT SNAPSHOT: Investigating {request.url}")
    
    # this is used in the rest of the time to poll for the submited build as a snapshot, knowing the snapshot view id and project view id
    if request.url == coverityAPI.snapshots_table_url:
        logger.debug(f"RECENT SNAPSHOT: Found snapshot table at {request.url}")
        with coverityAPI.interceptor_condition:
            
            # https://stackoverflow.com/questions/67306915/selenium-wire-response-object-way-to-get-response-body-as-string-rather-than-b
            snapshot_payload = json.loads(decode(response.body, response.headers.get('Content-Encoding', 'identity')))['resultSet']['results']

            if snapshot_payload == []:
                coverityAPI.cached_recent_snapshot = None
            else:
                coverityAPI.cached_recent_snapshot = snapshot_payload[0]

            logger.debug(f"Found most recent snapshot {coverityAPI.cached_recent_snapshot}")

            coverityAPI.recent_snapshot_state = RecentSnapshotStates.FOUND

            coverityAPI.interceptor_condition.notify()

    return

def defects_response_interceptor(request : Request, response : Response):

    coverityAPI : CoverityAPI = CoverityAPI(None)

    logger.debug(f"RECENT DEFECTS: Investigating {request.url}")

    if coverityAPI.defects_state in [DefectsStates.NOT_STARTED, DefectsStates.FOUND]:
        logger.debug(f"DEFECTS: Ignoring {request.url} for recent defects since status is {coverityAPI.defects_state.name}")
        return
    
    logger.debug(f"RECENT DEFECTS: Investigating {request.url}")
    current_defect_view_match = re.match(r"https://scan9\.scan\.coverity\.com/reports/table\.json\?projectId=(\d+)&viewId=(\d+)", request.url)

    if current_defect_view_match:
        
        coverityAPI.current_defects_view_id = current_defect_view_match.group(2)

        logger.debug(f"RECENT DEFECTS: Found copy of the defects view with id {coverityAPI.current_defects_view_id} at {request.url}")

        with coverityAPI.interceptor_condition:

            coverityAPI.cached_last_defect_results = json.loads(decode(response.body, response.headers.get('Content-Encoding', 'identity')))['resultSet']['results']

            for raw_defect in coverityAPI.cached_last_defect_results:
                logger.debug(raw_defect)

            coverityAPI.defects_state = DefectsStates.FOUND

            coverityAPI.interceptor_condition.notify()
    
    return

class CoverityAPI(object):

    user_email : str
    user_pass : str
    upload_token : str
    browser : Firefox
    scraper_wait : int
    project_overview_url : str
    project_id : int
    snapshots_url : str
    snapshots_table_url : str
    snapshots_view_id : int
    current_defects_view_id : int
    recent_snapshot_state : RecentSnapshotStates
    defects_state : DefectsStates
    interceptor_condition : Condition
    cached_last_defect_results : list[dict]
    cached_recent_snapshot : dict
    interceptor_timeout : int
    is_auth : bool
    log_file : str
    upload_token : str
    user_email : str
    project_name : str
    unknown_snapshot : bool
    snapshot_polling_seconds : int

    def __new__(cls, configs : dict | None) -> None:
        if not hasattr(cls, 'instance') and configs != None:
            
            cls.instance : CoverityAPI = super(CoverityAPI, cls).__new__(cls)

            cls.instance.log_file = configs['logfile']
            cls.instance.upload_token = os.environ[configs['coverityAPI']['uploadTokenEnv']]
            cls.instance.user_email = os.environ[configs["coverityAPI"]['userEmailEnv']]
            cls.instance.project_name = configs['coverityAPI']['projectName']

            cls.instance.unknown_snapshot = True
            cls.instance.snapshot_polling_seconds = configs["coverityAPI"]['snapshotPollingSeconds']
            
            cls.instance.options = Options()
            cls.instance.options.binary_location = configs["coverityAPI"]["firefoxPath"]
            gecko_service = Service(configs["coverityAPI"]["geckoPath"])
            cls.instance.browser = webdriver.Firefox(options=cls.instance.options, service=gecko_service)

            cls.instance.interceptor_condition = Condition()

            cls.instance.browser.request_interceptor = dummy_request_interceptor

            cls.instance.recent_snapshot_state = RecentSnapshotStates.NOT_STARTED
            cls.instance.defects_state = DefectsStates.NOT_STARTED

            cls.instance.scraper_wait = configs["coverityAPI"]["scraperWaitSeconds"]
            cls.instance.project_overview_url = configs["coverityAPI"]["projectOverviewURL"]

            cls.instance.interceptor_timeout = configs["coverityAPI"]["interceptorTimeoutSeconds"]

            cls.instance.is_auth = False

        return cls.instance
    
    def auth(self):

        browser = self.browser

        browser.switch_to.new_window('tab')

        browser.get(self.project_overview_url)

        try:
            # wait until "View Defects" button can be seen and clicked. This means that we wait until the user MANUALLY logs in and solves the captcha
            WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, "/html/body/div[1]/div[2]/div/div/div[3]/div[1]/p[1]/a")))
        except TimeoutException as te:
            self.is_auth = False
            raise te
        
        self.is_auth = True

        return

    def prepare_defects_for_db(self, defect : dict, compilation_tag) -> dict:

        # just remove the / from the beggining of the path, Coverity adds its since the archive submited is considered to be the whole filesystem
        defect['displayFile'] = defect['displayFile'][1 : ]

        # add compilation tag to the defect dict
        defect['compilation_tag'] = compilation_tag

        return defect
    
    def poll_recent_snapshot(self, compilation_tag : str | None) -> bool:

        if not self.is_auth:
            logger.warning("Authenticating before checking the most recent snapshot")
            try:
                self.auth()
            except TimeoutException as te:
                logger.critical("Authentication failed during fetching the most recent snapshot")
                raise te

        browser = self.browser
        
        if compilation_tag != None:
            logger.debug("Start intercepting for snapshot resultSet response")
        else:
            logger.debug("Start searching for snapshots view id and project view id as the first step")
        
        browser.switch_to.new_window('tab')

        coverity_project_name = self.project_overview_url.split("/")[-1].split("?")[0]

        coverity_view_defects_url = f"https://scan.coverity.com/projects/{coverity_project_name}/view_defects"

        browser.get(coverity_view_defects_url)

        logger.info(f"Request GET for {coverity_view_defects_url} for snapshot investigation")

        try:
            more_options = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="views-button"]')))
            more_options.click()
        except TimeoutException as te:
            logger.critical("Cannot click on more options burger button")
            raise te

        logger.debug("Clicked on more options. Next choose `All in project` snapshots option")
        time.sleep(1)

        try:
            # "All in project" snapshot button should be searched by CSS instead of XPATH, sicne users cand add other buttons before it and break the XPATH ordering
            all_in_snapshot = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, "/html/body/cim-root/cim-shell/div/cim-sidebar/nav/div/div[9]/div/ul/li[1]/div/a")))

            # this is the first time calling this function so we need to find the snapshot view id and project view id
            if self.unknown_snapshot:
                all_in_snapshot_element = browser.find_element(By.XPATH, "/html/body/cim-root/cim-shell/div/cim-sidebar/nav/div/div[9]/div/ul/li[1]/div/a")

                snapshots_href = all_in_snapshot_element.get_attribute("href")
                
                self.project_id = snapshots_href.split("/")[-1]

                self.snapshots_view_id = snapshots_href.split("/")[-2]

                self.unknown_snapshot = False

                logger.info(f"Snapshot view id is {self.snapshots_view_id} and project view id is {self.project_id}")

                # this url is accessible and will trigger a request to the second one which contains snapshots data
                self.snapshots_url = f"https://scan9.scan.coverity.com/#/project-view/{self.snapshots_view_id}/{self.project_id}"
                self.snapshots_table_url = f"https://scan9.scan.coverity.com/reports/table.json?projectId={self.project_id}&viewId={self.snapshots_view_id}"


                return True

            # init interceptor which will search for snapshot information in the response body
            else:
                self.recent_snapshot_state = RecentSnapshotStates.STARTED_BUT_NOT_FOUND
                browser.response_interceptor = recent_snapshot_response_interceptor

            all_in_snapshot.click()

            logger.debug("Clicked on `All in project` snapshot option")

        except TimeoutException as te:
            logger.critical("Cannot click on the `All in project` snapshot option")
            raise te
        

        # wait until the interceptor finds the response that contains the snapshot details
        with self.interceptor_condition:
            while self.recent_snapshot_state != RecentSnapshotStates.FOUND:
                self.interceptor_condition.wait(self.interceptor_timeout)

                # timeout inside the interceptor
                if self.recent_snapshot_state != RecentSnapshotStates.FOUND:
                    logger.critical(f"Interceptor timeout. Expected {RecentSnapshotStates.FOUND.name}, got {self.recent_snapshot_state.name}")
                    raise InterceptorTimeout()
                
            # reinit the state of the snapshots interceptor maybe for further use
            self.recent_snapshot_state = RecentSnapshotStates.NOT_STARTED

            # stop interceptor so that we do not pollute the log file too much
            self.browser.response_interceptor = dummy_response_interceptor
            logger.debug("RECENT SNAPSHOT: Interceptor stopped")

        if compilation_tag == None:
            logger.debug("Initial polling OK. Finished finding snapshot view id and project view id")
            return True
        
        if self.cached_recent_snapshot['snapshotDescription'] != compilation_tag:
            logger.warning(f"Recent snapshot has compilation tag \"{self.cached_recent_snapshot['snapshotDescription']}\", expected \"{compilation_tag}\". Needs retrying.")

            # the build has not been analyzed yet, we need to wait before retrying to get the most recent snpashot containing the build
            time.sleep(self.snapshot_polling_seconds)
            return False
        else:
            logger.warning(f"Found uploaded snapshot with tag {compilation_tag}")
        

        return True
    
    def init_snapshot_and_project_views(self):
        self.poll_recent_snapshot(None)

    def fetch_and_cache_recent_defects(self, compilation_tag : str) -> None:
        
        import time

        if not self.is_auth:
            logger.warn("Authenticating before fetching most recent defects...")
            try:
                self.auth()
            except TimeoutException as te:
                logger.critical("Authentication failed during fetching defects in the most recent snapshot")
                raise te
            
        browser = self.browser

        # try to intercept response body which contains all snapshot information
        browser.switch_to.new_window('tab')

        browser.get(self.snapshots_url)
        
        try:
            recent_snapshot_cell = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/cim-root/cim-shell/div/cim-project-view/div[1]/div[1]/div[1]/cim-data-table/angular-slickgrid/div/div/div[4]/div[3]/div/div[1]/div[2]')))
            double_click_snapshot_cell = ActionChains(browser).double_click(recent_snapshot_cell).perform()
        except TimeoutException as te:
            logger.critical("Cannot find most recent snapshot cell when fetching defects")
            raise te
        time.sleep(2)
        # Add more columns
        try:
            edit_settings_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="current-selection-settings"]')))
            edit_settings_button.click()
        except TimeoutException as te:
                logger.critical("Cannot find `edit settings` button")
                raise te
        
        logger.debug("Opened settings in order to add additional columns in the snapshot view. Next click on `Save as Copy` button")
        time.sleep(2)

        try:
            save_as_copy_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/simple-modal-holder/simple-modal-wrapper/div/cim-setting-modal/cim-base-modal/div/div[2]/div/div/form/ul/li[2]/label')))
            save_as_copy_button.click()
        except TimeoutException as te:
            logger.critical("Cannot click on `Save as Copy` button")
            raise te

        logger.debug("Clicked on `Save as Copy` button. Next click no `Column` button")
        time.sleep(2)

        try:
            column_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/simple-modal-holder/simple-modal-wrapper/div/cim-setting-modal/cim-base-modal/div/div[2]/div/div/fieldset/div/ul/li[2]/a/span')))
            column_button.click()
        except TimeoutException as te:
            logger.critical("Cannot click on `Column` button")
            raise te

        logger.debug("Clicked on column button. Next choose `Checker` column")
        time.sleep(2)

        try:
            checker_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-checker"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", checker_column)
            time.sleep(1)
            checker_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `Checker` column")
            raise te

        logger.debug("Clicked on `Checker` column. Next choose `CWE` column")
        time.sleep(2)

        try:
            cwe_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-cwe"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", cwe_column)
            time.sleep(1)
            cwe_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `CWE` column")
            raise te
        
        logger.debug("Clicked on `CWE` column. Next choose `Line Number` column")
        time.sleep(2)

        try:
            line_number_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-lineNumber"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", line_number_column)
            time.sleep(1)
            line_number_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `Line Number` column")
            raise te
        
        logger.debug("Clicked on `Line Number` column. Next choose `Score` column")
        time.sleep(2)

        try:
            score_column = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="select-score"]')))
            browser.execute_script("arguments[0].scrollIntoView(true);", score_column)
            time.sleep(1)
            score_column.click()
        except TimeoutException as te:
            logger.critical("Cannot choose `Score` column")
            raise te

        logger.debug("Clicked on `Score` column. Next enter name of the snapshot issue copy")
        time.sleep(2)

        try:
            snapshot_copy_input = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="view-name"]')))
            snapshot_copy_input.clear()
            snapshot_copy_input.send_keys(compilation_tag)
        except TimeoutException as te:
            logger.critical("Cannot input the name of the snapshot issue copy")
            raise te

        logger.debug("Chose a name for the snapshot issue copy. Next click on `OK` button to finish and start intercepting the response body for defects")
        time.sleep(2)

        self.defects_state = DefectsStates.STARTED_BUT_NOT_FOUND
        browser.response_interceptor = defects_response_interceptor

        try:
            ok_button = WebDriverWait(browser, self.scraper_wait).until(EC.element_to_be_clickable((By.XPATH, '/html/body/simple-modal-holder/simple-modal-wrapper/div/cim-setting-modal/cim-base-modal/div/div[3]/div/div/button[2]')))

            logger.debug("Init defects response interceptor")
            # only after initialising the interceptor, we can click on 'OK'
            ok_button.click()
        except TimeoutException as te:
            logger.critical("Cannot click on `OK` button to finish adding more columns")
            raise te

        logger.debug("Finished adding new columns. Now try to intercept the response ")

        with self.interceptor_condition:
            while self.defects_state != DefectsStates.FOUND:
                self.interceptor_condition.wait(self.interceptor_timeout)

                # timeout inside the interceptor
                if self.defects_state != DefectsStates.FOUND:
                    logger.critical(f"Interceptor timeout. Expected {DefectsStates.FOUND.name}, got {self.defects_state}")
                    raise InterceptorTimeout()
                
            # reinit the state of the defects interceptor maybe for further use
            self.defects_state = DefectsStates.NOT_STARTED
            
        return

    def submit_build(self, app_path : str, compile_cmd : str, compilation_tag : str) -> bool:
        
        if os.path.exists(f"{app_path}/.unikraft"):
            logger.warning("Detected already built app. Removing the .unikraft directory")
            shutil.rmtree(f"{app_path}/.unikraft")

        upload_script_path = os.getcwd() + "/.."

        job = f"{upload_script_path}/upload.sh {app_path} \"{compile_cmd}\" {self.upload_token} {self.user_email} \"{compilation_tag}\" \"{self.project_name}\" >> {self.log_file}"

        logger.debug(f"Executing upload job: {upload_script_path}/upload.sh")

        submit_proc = subprocess.Popen(job, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        _, err_raw = submit_proc.communicate()

        err = err_raw.decode()
        
        submit_proc.terminate()

        logger.warning(err)

        if "No files were emitted." in err:
            logger.critical("The app was previously compiled. Remove the .unikraft directory and try again")
            return False
        
        compilation_success_msg = "C/C++ compilation units (100%) are ready for analysis"

        # search through the log file for compilation status, accept only 100% compilation ratio
        compile_status_proc = subprocess.Popen(f'grep \"{compilation_success_msg}\" {self.log_file}', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = compile_status_proc.communicate()
        if out == b'':
            logger.critical(f"The app has compilation issues. The compilation ration should be 100% in order to you the compilation coverage tool")
            return False
        if err != b'':
            logger.warning(f"Error warning while grep-ing for 100% compile ration:\n{err.decode()}")
        
        # Coverity limits the number of builds you upload
        quota_msg = "The build submission quota for this project has been reached."
        quota_reached_proc = subprocess.Popen(f'grep \"{quota_msg}\" {self.log_file}', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out,err = quota_reached_proc.communicate()
        if out != b'':
            logger.critical("Quota for Coverity submited builds reached. Try again next time")
            logger.critical(out)
            return False
        if err != b'':
            logger.warning(f"Error warning while grep-ing for reached quota:\n{err.decode()}")
            

        return True
                    
